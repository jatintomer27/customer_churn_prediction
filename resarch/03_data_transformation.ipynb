{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ed2a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "839a6eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jatin/Projects/customer_churn_prediction'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c67f1814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation component\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    \"\"\"\n",
    "    Storing configuration related to the data transformation.\n",
    "    \"\"\"\n",
    "    root_dir: Path\n",
    "    local_data_file: Path\n",
    "    filtered_data_file: Path\n",
    "    encoded_data_file: Path\n",
    "    encoder_file: Path\n",
    "    schema: dict\n",
    "    target_column: dict\n",
    "    params: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95d9eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from customer_churn_prediction.constants import *\n",
    "from customer_churn_prediction.utils.common import read_yaml, create_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a1e74eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-17 21:57:22,810]:INFO:common.py:Yaml file: schema.yaml is loaded successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_items([('name', 'Churn'), ('type', 'object')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_yaml(SCHEMA_FILE_PATH).TARGET_COLUMN.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "379be3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from customer_churn_prediction.constants import *\n",
    "from customer_churn_prediction.utils.common import read_yaml, create_directory\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_path=CONFIG_FILE_PATH,\n",
    "            schema_path=SCHEMA_FILE_PATH,\n",
    "            params_path=PARAMS_FILE_PATH\n",
    "    ):\n",
    "        \n",
    "        self.config = read_yaml(config_path)\n",
    "        self.schema = read_yaml(schema_path)\n",
    "        self.params = read_yaml(params_path)\n",
    "\n",
    "        create_directory([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self)-> DataTransformationConfig:\n",
    "        \"\"\"\n",
    "        Return data transformation config\n",
    "        \"\"\"\n",
    "        config = self.config.data_transformation\n",
    "        schema = self.schema.COLUMNS\n",
    "        target_column = self.schema.TARGET_COLUMN\n",
    "        params = self.params\n",
    "        create_directory([config.root_dir])\n",
    "        \n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            local_data_file=config.local_data_file,\n",
    "            filtered_data_file=config.filtered_data_file,\n",
    "            encoded_data_file=config.encoded_data_file,\n",
    "            encoder_file=config.encoder_file,\n",
    "            schema=schema,\n",
    "            target_column=target_column,\n",
    "            params=params\n",
    "        )\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10c33380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from customer_churn_prediction import logger\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def filter_dataset(self):\n",
    "        \"\"\"\n",
    "        Read the data from the raw stored file.\n",
    "        Select only the relevant columns and store it in another file.\n",
    "        \"\"\"\n",
    "        target_column = self.config.target_column.name\n",
    "        relevant_columns = list(self.config.schema.keys())\n",
    "        relevant_columns.append(target_column)\n",
    "        data = pd.read_csv(self.config.local_data_file)\n",
    "        final_data = data[relevant_columns]\n",
    "        final_data.to_csv(self.config.filtered_data_file,index=False)\n",
    "        logger.info(f\"Selected only relevant columns based on the schema and stored in {self.config.filtered_data_file}\")\n",
    "\n",
    "    def categorical_column_encoder(self,encoding):\n",
    "        \"\"\"\n",
    "        Encode the categorical features and target column.\n",
    "\n",
    "        Params:\n",
    "            encoding (str): The type of encoding want to apply on columns\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data = pd.read_csv(self.config.filtered_data_file)\n",
    "            columns = self.config.schema.items()\n",
    "            categorical_features = list(filter(lambda col: col[1] in ['str','object'],columns))\n",
    "            logger.info(f\"categorical_features {categorical_features}\")\n",
    "            encoders = {}\n",
    "            for column, d_type in categorical_features:\n",
    "                if encoding == 'label_encoding':\n",
    "                    label_encoder = LabelEncoder()\n",
    "                    data[column] = label_encoder.fit_transform(data[column])\n",
    "                    encoders[column] = label_encoder\n",
    "                elif encoding == 'one_hot_encoding':\n",
    "                    logger.info(f\"Encoding based on One Hot Encoder is not implemented yet.\")\n",
    "                else:\n",
    "                    logger.info(f\"Only Label Encoding is implemented yet for encoding categorical variables\")\n",
    "\n",
    "            if self.config.target_column.type in  ['str','object']:\n",
    "                column = self.config.target_column.name\n",
    "                label_encoder = LabelEncoder()\n",
    "                data[column] = label_encoder.fit_transform(data[column])\n",
    "                encoders[column] = label_encoder\n",
    "            data.to_csv(self.config.encoded_data_file,index=False)\n",
    "            with open(self.config.encoder_file,'wb') as f:\n",
    "                pickle.dump(encoders,f)\n",
    "        except Exception:\n",
    "            logger.exception(f\"Exception occured while encoding the categorical variables\")\n",
    "            raise\n",
    "        \n",
    "    def train_test_splitting(self):\n",
    "        \"\"\"\n",
    "        Split the data into training and test set.\n",
    "        The test_size is read from the params.yaml file\n",
    "        The random_state is read from the params.yaml file\n",
    "        \"\"\"\n",
    "        data = pd.read_csv(self.config.encoded_data_file)\n",
    "        train, test = train_test_split(\n",
    "            data,\n",
    "            test_size=self.config.params.test_size,\n",
    "            random_state=self.config.params.random_state\n",
    "        )\n",
    "        train.to_csv(os.path.join(self.config.root_dir,\"train.csv\"),index=False)\n",
    "        test.to_csv(os.path.join(self.config.root_dir,\"test.csv\"),index=False)\n",
    "        logger.info(\"Splitted data into training and test set\")\n",
    "        logger.info(f\"training data shape: {train.shape}\")\n",
    "        logger.info(f\"test data shape: {test.shape}\")\n",
    "\n",
    "    def handle_inbalanced_data(self):\n",
    "        smote_random_state = self.config.params.data_transformation.smote_random_state or 23\n",
    "        train_data = pd.read_csv(os.path.join(self.config.root_dir,\"train.csv\"))\n",
    "        x_train = train_data.drop(columns=[self.config.target_column.name])\n",
    "        y_train = train_data[self.config.target_column.name]\n",
    "        smote = SMOTE(random_state=smote_random_state)\n",
    "        x_train_res, y_train_res = smote.fit_resample(x_train,y_train)\n",
    "        train_resampled = pd.concat([x_train_res,y_train_res],axis=1)\n",
    "        train_resampled.to_csv(os.path.join(self.config.root_dir,\"train.csv\"),index=False)\n",
    "        logger.info(\"Applied SMOTE and saved resampled training data\")\n",
    "\n",
    "    def is_inbalanced(self, y, threshould=0.7):\n",
    "        \"\"\"\n",
    "        Check for the class inbalance in the target column based on the threshould.\n",
    "\n",
    "        Params:\n",
    "            y (pandas series): target column series.\n",
    "            thershould (float): Threshould to decide class inbalancy.\n",
    "        \"\"\"\n",
    "        class_counts = y.value_counts(normalize=True)\n",
    "        minimum_class_ratio = class_counts.min()\n",
    "        return minimum_class_ratio < threshould\n",
    "    \n",
    "    def manage_inbalanced_data(self):\n",
    "        \"\"\"\n",
    "        Check and handle for the inbalance class in the target column of training data.\n",
    "        \"\"\"\n",
    "        train_data = pd.read_csv(os.path.join(self.config.root_dir,\"train.csv\"))\n",
    "        y_train = train_data[self.config.target_column.name]\n",
    "        if self.is_inbalanced(y_train, self.config.params.data_transformation.smote_threshold):\n",
    "            self.handle_inbalanced_data()\n",
    "        else:\n",
    "            logger.info(\"Data is already balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a61bbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-17 21:57:26,077]:INFO:common.py:Yaml file: config/config.yaml is loaded successfully\n",
      "[2026-01-17 21:57:26,082]:INFO:common.py:Yaml file: schema.yaml is loaded successfully\n",
      "[2026-01-17 21:57:26,093]:INFO:common.py:Yaml file: params.yaml is loaded successfully\n",
      "[2026-01-17 21:57:26,095]:INFO:common.py:Directory created at: artifacts\n",
      "[2026-01-17 21:57:26,097]:INFO:common.py:Directory created at: artifacts/data_transformation\n",
      "[2026-01-17 21:57:26,232]:INFO:3173689160.py:Selected only relevant columns based on the schema and stored in artifacts/data_transformation/customer_churn_data.csv\n",
      "[2026-01-17 21:57:26,257]:INFO:3173689160.py:categorical_features [('gender', 'object'), ('Partner', 'object'), ('Dependents', 'object'), ('PhoneService', 'object'), ('MultipleLines', 'object'), ('InternetService', 'object'), ('OnlineSecurity', 'object'), ('OnlineBackup', 'object'), ('DeviceProtection', 'object'), ('TechSupport', 'object'), ('StreamingTV', 'object'), ('StreamingMovies', 'object'), ('Contract', 'object'), ('PaperlessBilling', 'object'), ('PaymentMethod', 'object')]\n",
      "[2026-01-17 21:57:26,362]:INFO:3173689160.py:Splitted data into training and test set\n",
      "[2026-01-17 21:57:26,363]:INFO:3173689160.py:training data shape: (5282, 19)\n",
      "[2026-01-17 21:57:26,365]:INFO:3173689160.py:test data shape: (1761, 19)\n",
      "[2026-01-17 21:57:26,442]:INFO:3173689160.py:Applied SMOTE and saved resampled training data\n"
     ]
    }
   ],
   "source": [
    "# Create the pipeline\n",
    "\n",
    "from customer_churn_prediction import logger\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(data_transformation_config)\n",
    "    data_transformation.filter_dataset()\n",
    "    data_transformation.categorical_column_encoder(encoding='label_encoding')\n",
    "    data_transformation.train_test_splitting()\n",
    "    data_transformation.manage_inbalanced_data()\n",
    "except Exception:\n",
    "    logger.exception(f\"Exception occured while executing the data transformation pipeline\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9021f333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb0cf55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae70b130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8a81fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "customer_churn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
